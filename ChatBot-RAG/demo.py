import bentoml
import ollama
from bentoml.io import Text

# Definir los modelos
EMBEDDING_MODEL = "hf.co/CompendiumLabs/bge-base-en-v1.5-gguf"
LANGUAGE_MODEL = "hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF"

# Cargar dataset
VECTOR_DB = []
with open("./cat-facts.txt", "r") as file:
    dataset = file.readlines()

# Funci贸n para agregar datos al vector DB
def add_chunk_to_database(chunk):
    embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)["embeddings"][0]
    VECTOR_DB.append((chunk, embedding))

for chunk in dataset:
    add_chunk_to_database(chunk)

# Funci贸n para calcular la similitud del coseno
def cosine_similarity(a, b):
    dot_product = sum([x * y for x, y in zip(a, b)])
    norm_a = sum([x ** 2 for x in a]) ** 0.5
    norm_b = sum([x ** 2 for x in b]) ** 0.5
    return dot_product / (norm_a * norm_b)

# Funci贸n para recuperar informaci贸n
def retrieve(query, top_n=3):
    query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)["embeddings"][0]
    similarities = [(chunk, cosine_similarity(query_embedding, embedding)) for chunk, embedding in VECTOR_DB]
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_n]

# Nueva forma de definir el servicio en BentoML
@bentoml.service(name="chatbot_service")
class ChatbotService:

    @bentoml.api  
    async def ask(self, query: str) -> str:
        retrieved_knowledge = retrieve(query)

        instruction_prompt = f"You are a helpful chatbot. Use only the following pieces of context:\n" + \
                             "\n".join([f" - {chunk}" for chunk, _ in retrieved_knowledge])

        response = ollama.chat(
            model=LANGUAGE_MODEL,
            messages=[{"role": "system", "content": instruction_prompt}, {"role": "user", "content": query}],
        )

        return response["message"]["content"]